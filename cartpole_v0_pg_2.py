# -*- coding: utf-8 -*-
"""CartPole-v0-PG-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NAHWFIzRR0bkwW5HNhFXdpbbBhZOEPIt
"""

'''
Install the libraries at the start only if running on colab. These and the wrap_env function defined further along with the methods before it 
are only used for rendering the environment on google colab.
Remove those methods if running on your local system, and simply initiate as gym.make() instead of wrap_env(gym.make())
'''
!pip install gym pyvirtualdisplay > /dev/null 2>&1

!nvidia-smi

!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1

!apt-get update > /dev/null 2>&1
!apt-get install cmake > /dev/null 2>&1
!pip install --upgrade setuptools 2>&1
!pip install ez_setup > /dev/null 2>&1
!pip install gym > /dev/null 2>&1

# Commented out IPython magic to ensure Python compatibility.
import gym
from gym import logger as gymlogger
from gym.wrappers import Monitor
gymlogger.set_level(40) #error only
import numpy as np
import random
import matplotlib
import matplotlib.pyplot as plt
# %matplotlib inline
import math
import glob
import io
import base64
from IPython.display import HTML

from IPython import display as ipythondisplay

from pyvirtualdisplay import Display
display = Display(visible=0, size=(1400, 900))
display.start()

"""
Utility functions to enable video recording of gym environment and displaying it
To enable video, just do "env = wrap_env(env)""
"""

def show_video():
  mp4list = glob.glob('video/*.mp4')
  if len(mp4list) > 0:
    mp4 = mp4list[0]
    video = io.open(mp4, 'r+b').read()
    encoded = base64.b64encode(video)
    ipythondisplay.display(HTML(data='''<video alt="test" autoplay 
                loop controls style="height: 400px;">
                <source src="data:video/mp4;base64,{0}" type="video/mp4" />
             </video>'''.format(encoded.decode('ascii'))))
  else: 
    print("Could not find video")
    

def wrap_env(env):
  env = Monitor(env, './video', force=True)
  return env

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow as tf

env=wrap_env(gym.make('CartPole-v0'))

action_size=env.action_space.n
with tf.name_scope('input_'):
  inputs=tf.placeholder(tf.float32,[None,4],name='inputs')
  actions=tf.placeholder(tf.int32,[None,action_size],name='actions')
  disc_rewards=tf.placeholder(tf.float32,[None,],name='disc_rewards')

  fc1=tf.keras.layers.Dense(64,activation='relu')(inputs)
  fc2=tf.keras.layers.Dense(32,activation='relu')(fc1)
  fc3=tf.keras.layers.Dense(16,activation='relu')(fc2)
  fc4=tf.keras.layers.Dense(action_size,activation='softmax')(fc3)
  out=tf.nn.softmax(fc4)

  loss=tf.nn.softmax_cross_entropy_with_logits_v2(labels=actions,logits=out)
  weighted_loss=loss*disc_rewards
  final_loss=tf.reduce_mean(weighted_loss)
  trainer=tf.train.AdamOptimizer(0.01).minimize(final_loss)

def disc_norm(rewards):
  disc=np.zeros_like(rewards)
  sum=0
  gamma=0.95
  for i in reversed(range(len(rewards))):
    sum=gamma*sum+rewards[i]
    disc[i]=sum
  mean=np.mean(disc)
  std=np.std(disc)
  disc=(disc-mean)/std
  return disc

max_episodes=10
episode=0
time_steps=100
states=[]
actions=[]
rewards=[]
reward=0
total_reward=0
all_rewards=[]
action_size=env.action_space.n
with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  for episode in range(max_episodes):
    total_reward_ep=0
    state=env.reset()
    while True:
      env.render()
      action_prob=sess.run(out,feed_dict={inputs:state.reshape([1,4])})
      action = np.random.choice(range(action_prob.shape[1]), p=action_prob.ravel())
      next_state,reward,done,_=env.step(action)
      action_=np.zeros(action_size)
      action_[action]=1
      states.append(state)
      rewards.append(reward)
      actions.append(action_)
      state=next_state
      if done:
        break
    show_video()
    discounted_normalized_rewards=disc_norm(rewards)
    loss_, _ = sess.run([final_loss, trainer], feed_dict={inputs: np.vstack(np.array(states)),
                                                                 actions: np.vstack(np.array(actions)),
                                                                 disc_rewards: discounted_normalized_rewards
                                                                })
    total_reward_ep=np.sum(rewards)
    all.rewards.append(total_reward_ep)
    total_reward=np.sum(all_rewards)
    mean_reward=total_reward/(ep+1)
    print('')
    print('====================================')
    print('Total reward for {} ep is: {}'.format(episode, total_reward_ep))
    print('Total reward so far:{}'.format(total_reward))
    print('mean reward:{}'.format(mean_reward))
    print('loss:{}'.format(loss))
    print('====================================')

